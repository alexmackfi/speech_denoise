{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prepare our dataset for trainig","metadata":{}},{"cell_type":"markdown","source":"# Class for feature extraction","metadata":{}},{"cell_type":"code","source":"%%writefile feature_extractor.py\n\nimport librosa\nimport scipy\n\n\nclass FeatureExtractor:\n    def __init__(self, audio, *, windowLength, overlap, sample_rate):\n        self.audio = audio\n        self.ffT_length = windowLength\n        self.window_length = windowLength\n        self.overlap = overlap\n        self.sample_rate = sample_rate\n        self.window = scipy.signal.hamming(self.window_length, sym=False)\n\n    def get_stft_spectrogram(self):\n        return librosa.stft(self.audio, n_fft=self.ffT_length, win_length=self.window_length, hop_length=self.overlap,\n                            window=self.window, center=True)\n\n    def get_audio_from_stft_spectrogram(self, stft_features):\n        return librosa.istft(stft_features, win_length=self.window_length, hop_length=self.overlap,\n                             window=self.window, center=True)\n\n    def get_mel_spectrogram(self):\n        return librosa.feature.melspectrogram(self.audio, sr=self.sample_rate, power=2.0, pad_mode='reflect',\n                                              n_fft=self.ffT_length, hop_length=self.overlap,\n                                              win_length=self.window_length, window=self.window,center=True)\n\n    def get_audio_from_mel_spectrogram(self, M):\n        return librosa.feature.inverse.mel_to_audio(M, sr=self.sample_rate, n_fft=self.ffT_length,\n                                                    hop_length=self.overlap,\n                                                    win_length=self.window_length, window=self.window,\n                                                    center=True, pad_mode='reflect', power=2.0, n_iter=32, length=None)","metadata":{"_uuid":"2421e626-1585-4df0-9e4c-81dc3601a08a","_cell_guid":"ef652920-3387-4262-8655-20b3ff30b51d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Our dataset: get clean and noisy files in waveform","metadata":{}},{"cell_type":"code","source":"\n%%writefile findataset.py\nimport os\nfrom torch.utils.data import DataLoader, Dataset\nimport librosa\nimport numpy as np\nimport math\nfrom feature_extractor import FeatureExtractor\nfrom utils import prepare_input_features\nimport multiprocessing\nimport os\nfrom utils import get_tf_feature, read_audio\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n\nclass FinDataset(Dataset):\n    def __init__(self, clean_filenames, noise_filenames, **config):\n        self.clean_filenames = clean_filenames #чистые файлы\n        self.noise_filenames = noise_filenames #dirty files\n        self.sample_rate = config['fs'] #settings for spectrogramm\n        self.overlap = config['overlap']\n        self.window_length = config['windowLength']\n        self.audio_max_duration = config['audio_max_duration']\n        \n\n    def _sample_noise_filename(self):\n        return np.random.choice(self.noise_filenames) #choose random noise\n\n    def _remove_silent_frames(self, audio): #audio without silent\n        trimed_audio = [] \n        indices = librosa.effects.split(audio, hop_length=self.overlap, top_db=20)\n\n        for index in indices:\n            trimed_audio.extend(audio[index[0]: index[1]])\n        return np.array(trimed_audio)\n\n    def _phase_aware_scaling(self, clean_spectral_magnitude, clean_phase, noise_phase): #в зависимости от фазы преобразуем амплитуду\n        assert clean_phase.shape == noise_phase.shape, \"Shapes must match.\"\n        return clean_spectral_magnitude * np.cos(clean_phase - noise_phase)\n\n    def get_noisy_audio(self, *, filename): \n        return read_audio(filename, self.sample_rate)\n\n    def _audio_random_crop(self, audio, duration): #случайно вырезаем из аудио аудио заданной длины\n        audio_duration_secs = librosa.core.get_duration(audio, self.sample_rate)\n\n        ## duration: length of the cropped audio in seconds\n        audio_duration_ms = math.floor(audio_duration_secs * self.sample_rate)\n        duration_ms = math.floor(duration * self.sample_rate)\n        if duration_ms >= audio_duration_ms:\n            print(\"Passed duration greater than audio duration of: \", audio_duration_ms)\n            audio = np.append(audio, np.zeros(duration_ms-audio_duration_ms+5))\n        idx = np.random.randint(0, audio_duration_ms - duration_ms)\n        return audio[idx: idx + duration_ms]\n\n    def _add_noise_to_clean_audio(self, clean_audio, noise_signal): ##добавляем шум в чистое аудио\n        if len(clean_audio) >= len(noise_signal):\n            # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n            while len(clean_audio) >= len(noise_signal):\n                noise_signal = np.append(noise_signal, noise_signal)\n\n        ## Extract a noise segment from a random location in the noise file\n        ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n        noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n        speech_power = np.sum(clean_audio ** 2)\n        noise_power = np.sum(noiseSegment ** 2)\n        noisyAudio = clean_audio + (np.sqrt(speech_power / noise_power) * noiseSegment * np.random.sample())\n        #noisyAudio = clean_audio + (np.sqrt(speech_power / noise_power) * noiseSegment * np.random.sample())\n        return noisyAudio\n\n    def _parallel_audio_processing(self, clean_filename): ##Обрабатываем одновременно шумное и чистое аудио: читаем, удаляем тишину, \n## смешиваем, получаем смешанную sftf спектрограмму, ее амплитуду и фазу для чистого и смешанного аудио, нормируем амплитуду\n        clean_audio, _ = read_audio(clean_filename, self.sample_rate)\n\n        # remove silent frame from clean audio\n        #clean_audio = self._remove_silent_frames(clean_audio)\n\n        noise_filename = self._sample_noise_filename()\n\n        # read the noise filename\n        noise_audio, sr = read_audio(noise_filename, self.sample_rate)\n\n        # remove silent frame from noise audio\n        noise_audio = self._remove_silent_frames(noise_audio)\n\n        # sample random fixed-sized snippets of audio\n        clean_audio = self._audio_random_crop(clean_audio, duration=self.audio_max_duration)\n\n        # add noise to input image\n        noiseInput = self._add_noise_to_clean_audio(clean_audio, noise_audio)\n\n        # extract stft features from noisy audio\n        '''\n        noisy_input_fe = FeatureExtractor(noiseInput, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        noise_spectrogram = noisy_input_fe.get_stft_spectrogram()\n\n        # Or get the phase angle (in radians)\n        # noisy_stft_magnitude, noisy_stft_phase = librosa.magphase(noisy_stft_features)\n        noise_phase = np.angle(noise_spectrogram)\n\n        # get the magnitude of the spectral\n        noise_magnitude = np.abs(noise_spectrogram)\n\n        # extract stft features from clean audio\n        clean_audio_fe = FeatureExtractor(clean_audio, windowLength=self.window_length, overlap=self.overlap,\n                                          sample_rate=self.sample_rate)\n        clean_spectrogram = clean_audio_fe.get_stft_spectrogram()\n        # clean_spectrogram = cleanAudioFE.get_mel_spectrogram()\n\n        # get the clean phase\n        clean_phase = np.angle(clean_spectrogram)\n\n        # get the clean spectral magnitude\n        clean_magnitude = np.abs(clean_spectrogram)\n        # clean_magnitude = 2 * clean_magnitude / np.sum(scipy.signal.hamming(self.window_length, sym=False))\n\n        clean_magnitude = self._phase_aware_scaling(clean_magnitude, clean_phase, noise_phase)\n        mean = np.mean(noise_magnitude)\n        std = np.std(noise_magnitude)\n        scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n        noise_magnitude = scaler.fit_transform(noise_magnitude)\n        clean_magnitude = scaler.transform(clean_magnitude)\n        return noise_magnitude, clean_magnitude, noise_phase\n   '''     \n        return noiseInput, clean_audio\n    \n    def __getitem__(self, index):\n        clean = self.clean_filenames[index]\n        noiseInput, clean = self._parallel_audio_processing(clean)\n        return noiseInput, clean\n    \n    def __len__(self):\n        l = len(self.clean_filenames)\n        return l\n    \n    '''\n    def __getitem__(self, index):\n        clean = self.clean_filenames[index]\n        noise_m, clean_m, noise_ph = self._parallel_audio_processing(clean)\n        noise_m_f = prepare_input_features(noise_m, numSegments=8, numFeatures=129)\n        noise_m_f = np.transpose(noise_m_f, (2, 0, 1))\n        clean_m = np.transpose(clean_m, (1, 0))\n        noise_ph = np.transpose(noise_ph, (1, 0))\n        noise_m_f = np.expand_dims(noise_m_f, axis=1)\n        clean_m = np.expand_dims(clean_m, axis=2)\n        clean_m = np.expand_dims(clean_m, 1)\n        return noise_m_f, clean_m, noise_ph '''\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset for clean audio filenames","metadata":{}},{"cell_type":"code","source":"%%writefile mozilla_commonvoice.py\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n# np.random.seed(9)\n\nclass MozillaCommonVoiceDataset:\n\n    def __init__(self, basepath, *, val_dataset_size):\n        self.basepath = basepath\n        self.val_dataset_size = val_dataset_size\n\n    def get_common_voice_filenames(self, subfolder=\"train\", dataframe_name='train.tsv'):\n        full_file_path = os.path.join(self.basepath, subfolder, dataframe_name)\n        print(\"full path\", full_file_path)\n\n        mozilla_metadata = pd.read_csv(full_file_path, sep='\\t')\n        clean_files = mozilla_metadata['path'].values\n        np.random.shuffle(clean_files)\n        print(\"Total number of training examples:\", len(clean_files))\n        return clean_files\n\n    def get_train_val_filenames(self):\n        clean_files = self.get_common_voice_filenames(subfolder=\"train\", dataframe_name='train.tsv')\n\n        # resolve full path\n        clean_files = [os.path.join(self.basepath, 'train', 'clips', filename+\".wav\") for filename in clean_files]\n\n        clean_files = clean_files[:-self.val_dataset_size]\n        clean_val_files = clean_files[-self.val_dataset_size:]\n        print(\"# of Training clean files:\", len(clean_files))\n        print(\"# of  Validation clean files:\", len(clean_val_files))\n        return clean_files, clean_val_files\n\n\n    def get_test_filenames(self):\n        clean_files = self.get_common_voice_filenames(subfolder=\"test\", dataframe_name='test.tsv')\n\n        # resolve full path\n        clean_files = [os.path.join(self.basepath, 'test', 'clips', filename+\".wav\") for filename in clean_files]\n\n        print(\"# of Testing clean files:\", len(clean_files))\n#         print(\"Clean Test Files: \", clean_files)\n        return clean_files","metadata":{"_uuid":"aee205a6-c225-409d-8eb1-fbd5d476e33a","_cell_guid":"4a695899-cdb0-46e8-8b69-45805273f062","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset for noise audio filenames","metadata":{}},{"cell_type":"code","source":"%%writefile urban_sound_8k.py\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n#np.random.seed(9)\n\n\nclass UrbanSound8K:\n    def __init__(self, basepath, *, val_dataset_size, class_ids=None):\n        self.basepath = basepath\n        self.val_dataset_size = val_dataset_size\n        self.class_ids = class_ids\n\n    def _get_urban_sound_8K_filenames(self):\n        urbansound_metadata = pd.read_csv(os.path.join(self.basepath, 'UrbanSound8K.csv'))\n\n        # shuffle the dataframe\n        urbansound_metadata.reindex(np.random.permutation(urbansound_metadata.index))\n\n        return urbansound_metadata\n\n    def _get_filenames_by_class_id(self, metadata):\n\n        if self.class_ids is None:\n            self.class_ids = np.unique(metadata['classID'].values)\n            print(\"Number of classes:\", self.class_ids)\n\n        all_files = []\n        file_counter = 0\n        for c in self.class_ids:\n            per_class_files = metadata[metadata['classID'] == c][['slice_file_name', 'fold']].values\n            per_class_files = [os.path.join(self.basepath, 'fold' + str(file[1]), file[0]) for file in\n                               per_class_files]\n            print(\"Class c:\", str(c), 'has:', len(per_class_files), 'files')\n            file_counter += len(per_class_files)\n            all_files.extend(per_class_files)\n\n        assert len(all_files) == file_counter\n        return all_files\n\n    def get_train_val_filenames(self):\n        urbansound_metadata = self._get_urban_sound_8K_filenames()\n\n        # folds from 0 to 9 are used for training\n        urbansound_train = urbansound_metadata[urbansound_metadata.fold != 10]\n\n        urbansound_train_filenames = self._get_filenames_by_class_id(urbansound_train)\n        np.random.shuffle(urbansound_train_filenames)\n\n        # separate noise files for train/validation\n        urbansound_val = urbansound_train_filenames[-self.val_dataset_size:]\n        urbansound_train = urbansound_train_filenames[:-self.val_dataset_size]\n        print(\"Noise training:\", len(urbansound_train))\n        print(\"Noise validation:\", len(urbansound_val))\n\n        return urbansound_train, urbansound_val\n\n    def get_test_filenames(self):\n        urbansound_metadata = self._get_urban_sound_8K_filenames()\n\n        # fold 10 is used for testing only\n        urbansound_train = urbansound_metadata[urbansound_metadata.fold == 10]\n\n        urbansound_test_filenames = self._get_filenames_by_class_id(urbansound_train)\n        np.random.shuffle(urbansound_test_filenames)\n\n        print(\"# of Noise testing files:\", len(urbansound_test_filenames))\n        return urbansound_test_filenames","metadata":{"_uuid":"2eff8f80-40d5-4d73-b376-85dc85e45bdd","_cell_guid":"537b42b9-6936-4389-9cfe-77b983002265","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helpful function for our data","metadata":{}},{"cell_type":"code","source":"%%writefile utils.py \n\nimport numpy as np\nimport pickle\nimport librosa\n# import sounddevice as sd\nfrom pydub import AudioSegment\nimport IPython\nimport tensorflow as tf\n\n\ndef inverse_stft_transform(stft_features, window_length, overlap):\n    return librosa.istft(stft_features, win_length=window_length, hop_length=overlap)\n\n\ndef revert_features_to_audio(features, phase, window_length, overlap, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return inverse_stft_transform(features, window_length=window_length, overlap=overlap)\n\n\ndef play(audio, sample_rate):\n    # ipd.display(ipd.Audio(data=audio, rate=sample_rate))  # load a local WAV file\n    IPython.display.Audio(data=audio, rate=sample_rate)\n#     sd.play(audio, sample_rate, blocking=True)\n\n\ndef add_noise_to_clean_audio(clean_audio, noise_signal):\n    if len(clean_audio) >= len(noise_signal):\n        # print(\"The noisy signal is smaller than the clean audio input. Duplicating the noise.\")\n        while len(clean_audio) >= len(noise_signal):\n            noise_signal = np.append(noise_signal, noise_signal)\n\n    ## Extract a noise segment from a random location in the noise file\n    ind = np.random.randint(0, noise_signal.size - clean_audio.size)\n\n    noiseSegment = noise_signal[ind: ind + clean_audio.size]\n\n    speech_power = np.sum(clean_audio ** 2)\n    noise_power = np.sum(noiseSegment ** 2)\n    noisyAudio = clean_audio + np.sqrt(speech_power / noise_power) * noiseSegment\n    return noisyAudio\n\ndef read_audio(filepath, sample_rate, normalize=True):\n    audio, sr = librosa.load(filepath, sr=sample_rate)\n    if normalize is True:\n        div_fac = 1 / np.max(np.abs(audio)) / 3.0\n        audio = audio * div_fac\n        # audio = librosa.util.normalize(audio)\n    return audio, sr\n\n\ndef prepare_input_features(stft_features, numSegments, numFeatures):\n    noisySTFT = np.concatenate([stft_features[:, 0:numSegments - 1], stft_features], axis=1)\n    stftSegments = np.zeros((numFeatures, numSegments, noisySTFT.shape[1] - numSegments + 1))\n\n    for index in range(noisySTFT.shape[1] - numSegments + 1):\n        stftSegments[:, :, index] = noisySTFT[:, index:index + numSegments]\n    return stftSegments\n\n\ndef get_input_features(predictorsList):\n    predictors = []\n    for noisy_stft_mag_features in predictorsList:\n        inputFeatures = prepare_input_features(noisy_stft_mag_features)\n        predictors.append(inputFeatures)\n\n    return predictors\n\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _float_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef get_tf_feature(noise_stft_mag_features, clean_stft_magnitude, noise_stft_phase):\n    noise_stft_mag_features = noise_stft_mag_features.astype(np.float32).tostring()\n    clean_stft_magnitude = clean_stft_magnitude.astype(np.float32).tostring()\n    noise_stft_phase = noise_stft_phase.astype(np.float32).tostring()\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'noise_stft_phase': _bytes_feature(noise_stft_phase),\n        'noise_stft_mag_features': _bytes_feature(noise_stft_mag_features),\n        'clean_stft_magnitude': _bytes_feature(clean_stft_magnitude)}))\n    return example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Train Our Models","metadata":{"_uuid":"917be118-ff50-42e4-b09b-bebbe1df1cdf","_cell_guid":"16abf0e5-8807-4e0d-9a1e-f1b58c01af27","trusted":true}},{"cell_type":"markdown","source":"# Import libraries for training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport librosa\nimport pandas as pd\nimport os\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport IPython.display as ipd\nimport librosa.display\nimport scipy\nimport glob\nimport numpy as np\nimport math\nimport warnings\nimport pickle\nfrom sklearn.utils import shuffle\nfrom feature_extractor import FeatureExtractor\nfrom sklearn.preprocessing import StandardScaler\n# Load the TensorBoard notebook extension.\n%load_ext tensorboard","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyModelExpencive1(nn.Module):\n    def __init__(self):\n        super(MyModelExpencive1, self).__init__()\n        self.pad = nn.ZeroPad2d((0, 0, 3, 4))\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=(9, 16), stride=(1, 1), padding=(0, 0), bias=False)\n        self.relu = nn.PReLU()\n        self.batchnorm1 = nn.BatchNorm2d(32)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm3 = nn.BatchNorm2d(128)\n        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm4 = nn.BatchNorm2d(256)\n        self.conv5 = nn.Conv2d(256, 512, kernel_size=(3, 1), stride=(2, 1), padding=(1, 0), bias=False)\n        self.batchnorm5 = nn.BatchNorm2d(512)\n        \n        self.conv5_1 = nn.Conv2d(512, 1024, kernel_size=(3, 1), stride=(8, 1), padding=(1, 0), bias=False)\n        self.batchnorm5_1 = nn.BatchNorm2d(1024)\n        self.conv6_1 = nn.Conv2d(1024, 512, kernel_size=(3, 1), stride=(1, 8), padding=(1, 0), bias=False)\n        self.batchnorm6_1 = nn.BatchNorm2d(512)\n        \n        self.conv6 = nn.Conv2d(512, 256, kernel_size=(3, 1), stride=(1, 2), padding=(1, 0), bias=False)\n        self.batchnorm6 = nn.BatchNorm2d(256)\n        self.conv7 = nn.Conv2d(256, 128, kernel_size=(3, 1), stride=(1, 2), padding=(1, 0), bias=False)\n        #self.conv7 = nn.ConvTranspose2d(256, 128, kernel_size=(3, 1), stride=(2, 1), padding=(2, 0), bias=False)\n        self.batchnorm7 = nn.BatchNorm2d(128)\n        self.conv8 = nn.Conv2d(128, 64, kernel_size=(3, 1), stride=(1, 2), padding=(1, 0), bias=False)\n        self.batchnorm8 = nn.BatchNorm2d(64)\n        self.conv9 = nn.Conv2d(64, 32, kernel_size=(3, 1), stride=(1, 2), padding=(1, 0), bias=False)\n        self.batchnorm9 = nn.BatchNorm2d(32)\n        self.spatialdropout = nn.Dropout2d(0.2)\n        self.conv10 = nn.Conv2d(32, 1, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n        self.upsample = nn.Upsample(scale_factor=2, mode='bicubic')\n        self.upsample2 = nn.Upsample(scale_factor=8, mode='bicubic')\n        self.pad2 = nn.ZeroPad2d((0, 0, 1, 0))\n        self.pad3 = nn.ZeroPad2d((0, 0, 0, 1))\n        self.pad4 = nn.ZeroPad2d((0, 0, 1, 0))\n    def forward(self, x):\n        #print(x.size())\n        x = self.pad(x)\n        #print(x.size())\n        #x = self.conv1(x)\n        skip9 = self.conv1(x)\n        x = self.relu(skip9)\n        #print(x.size())\n        #x = self.batchnorm1(x)\n        skip8 = self.conv2(x)\n        x = self.relu(skip8)\n        #print(x.size())\n        x = self.batchnorm2(x)\n        skip7 = self.conv3(x)\n        x = self.relu(skip7)\n        #print(x.size())\n        x = self.batchnorm3(x)\n        skip6 = self.conv4(x)\n        x = self.relu(skip6)\n        #print(x.size())\n        x = self.batchnorm4(x)\n        skip6_1 = self.conv5(x)\n        #x = self.pad3(x)\n        #print(x.size())\n        x = self.relu(skip6_1)\n        #print(x.size())\n        #x = self.batchnorm5(x)\n        \n        x = self.conv5_1(x)\n        #x = self.pad3(x)\n        #print(x.size())\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm5_1(x)\n        x = self.upsample2(x)\n        x = self.conv6_1(x)\n        #print(x.size())\n        x = x + skip6_1\n        \n        \n        \n        x = self.upsample(x)\n        x = self.conv6(x)\n        #print(x.size())\n        x = x + skip6\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm6(x)\n        x = self.upsample(x)\n        #x = self.pad4(x)\n        x = self.conv7(x)\n        x = x + skip7\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm7(x)\n        x = self.upsample(x)\n        x = self.conv8(x)\n        #x = self.pad3(x)\n        x = x + skip8\n        x = self.relu(x)\n        #print(x.size())\n        x = self.batchnorm8(x)\n        x = self.upsample(x)\n        #x = self.pad4(x)\n        x = self.conv9(x)\n        x = x + skip9\n        x = self.relu(x)\n        #print(x.size())\n        #x = self.batchnorm9(x)\n        x = self.spatialdropout(x)\n        x = self.conv10(x)\n        x = self.pad2(x)\n        #print(x.size())\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MyModelLight(nn.Module):\n    def __init__(self):\n        super(MyModelLight, self).__init__()\n        self.pad = nn.ZeroPad2d((0, 0, 4, 4))\n        self.conv1 = nn.Conv2d(1, 8, kernel_size=(9, 16), stride=(1, 1), padding=(0, 0), bias=False)\n        self.relu = nn.PReLU()\n        self.batchnorm1 = nn.BatchNorm2d(8)\n        self.conv2 = nn.Conv2d(8, 18, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n        self.batchnorm2 = nn.BatchNorm2d(18)\n        self.conv3 = nn.Conv2d(18, 30, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), bias=False)\n        self.batchnorm3 = nn.BatchNorm2d(30)\n        self.conv4 = nn.Conv2d(30, 18, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), bias=False)\n        self.batchnorm4 = nn.BatchNorm2d(18)\n        self.conv5 = nn.Conv2d(18, 30, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n        self.batchnorm5 = nn.BatchNorm2d(30)\n        self.conv5_1 = nn.Conv2d(30, 120, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), bias=False)\n        self.batchnorm5_1 = nn.BatchNorm2d(120)\n        self.conv5_2 = nn.Conv2d(120, 30, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n        self.batchnorm5_2 = nn.BatchNorm2d(30)\n        self.conv6 = nn.Conv2d(30, 18, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), bias=False)\n        self.batchnorm6 = nn.BatchNorm2d(18)\n        self.conv7 = nn.Conv2d(18, 30, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), bias=False)\n        self.batchnorm7 = nn.BatchNorm2d(30)\n        self.conv8 = nn.Conv2d(30, 18, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n        self.batchnorm8 = nn.BatchNorm2d(18)\n        self.conv9 = nn.Conv2d(18, 8, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), bias=False)\n        self.batchnorm9 = nn.BatchNorm2d(8)\n        self.spatialdropout = nn.Dropout2d(0.2)\n        self.conv10 = nn.Conv2d(8, 1, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0))\n\n    def forward(self, x):\n        print(x.size())\n        x = self.pad(x)\n        print(x.size())\n        x = self.conv1(x)\n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm1(x)\n        skip8 = self.conv2(x)\n        x = self.relu(skip8)\n        print(x.size())\n        x = self.batchnorm2(x)\n        x = self.conv3(x)\n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm3(x)\n        x = self.conv4(x)\n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm4(x)\n        skip6 = self.conv5(x)\n        print(skip6.size())\n        x = self.relu(skip6)\n        print(x.size())\n        x = self.batchnorm5(x)\n        \n        x = self.conv5_1(x)\n        print(x.size())\n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm5_1(x)\n        x = self.conv5_2(x)\n        x = x +skip6\n        print(x.size())\n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm5_2(x)\n        \n        x = self.conv6(x)\n  \n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm6(x)\n        x = self.conv7(x) \n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm7(x)\n        x = self.conv8(x)\n        x = x + skip8\n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm8(x)\n        x = self.conv9(x)\n        x = self.relu(x)\n        print(x.size())\n        x = self.batchnorm9(x)\n        x = self.spatialdropout(x)\n        x = self.conv10(x)\n        print(x.size())\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get device","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n#print(torch.cuda.get_device_name())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install wandb","metadata":{}},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set constant's values","metadata":{}},{"cell_type":"code","source":"windowLength = 256\noverlap      = round(0.25 * windowLength) # overlap of 75%\nffTLength    = windowLength\ninputFs      = 48e3\nfs           = 16000\nnumFeatures  = ffTLength//2 + 1\nnumSegments  = 16\nprint(\"windowLength:\",windowLength)\nprint(\"overlap:\",overlap)\nprint(\"ffTLength:\",ffTLength)\nprint(\"inputFs:\",inputFs)\nprint(\"fs:\",fs)\nprint(\"numFeatures:\",numFeatures)\nprint(\"numSegments:\",numSegments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions for feature extraction","metadata":{}},{"cell_type":"code","source":"def revert_features_to_audio(features, phase, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n\n    # features = librosa.db_to_power(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return noiseAudioFeatureExtractor.get_audio_from_stft_spectrogram(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input_features(stft_features, numSegments, numFeatures):\n    noisySTFT = np.concatenate([stft_features[:, 0:numSegments - 1], stft_features], axis=1) #важная конкатенация\n\n    stftSegments = np.zeros((numFeatures, numSegments, noisySTFT.shape[1] - numSegments + 1))\n    for index in range(noisySTFT.shape[1] - numSegments + 1):\n        stftSegments[:, :, index] = noisySTFT[:, index:index + numSegments]\n    return stftSegments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"markdown","source":"# Install PESQ and STOI","metadata":{}},{"cell_type":"code","source":"!pip install pypesq\n!pip3 install pystoi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define SNR","metadata":{}},{"cell_type":"code","source":"def getPower(signal):\n\n    return np.sqrt(np.sum(signal**2)/np.size(signal))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getSNR(clean, noise):\n    speech_power = getPower(clean)\n    noise_power = getPower(noise)\n    \n    # snr between clean speech and noise\n    snrCNa = speech_power / noise_power\n    \n    # compute amplified noise\n    noiseAmp = snrCNa * noise\n    # amplified noise power\n    noiseAmp_power = getPower(noiseAmp)\n    \n    snrCNa = speech_power / noiseAmp_power\n    \n#     print(\"speech_power:\", speech_power)\n#     print(\"noise_power:\", noise_power)\n#     print(\"snr clean & noise:\", round(snrCNa, 3))\n    return snrCNa\n\n\n\ndef getSNR_dB(snrVal):\n    return 20*np.log10(snrVal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING AND VALIDATION ON EPOCH","metadata":{}},{"cell_type":"markdown","source":"# Train on epoch","metadata":{}},{"cell_type":"code","source":"def revert_features_to_audio(features, phase, noisy_input_fe, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n\n    # features = librosa.db_to_power(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return noisy_input_fe.get_audio_from_stft_spectrogram(features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport soundfile as sf\nfrom pypesq import pesq\nfrom pystoi import stoi\n\ndef train_on_epoch(MyModel, loss_fn, optimizer, train_loader, train_dataset, best_diff_stoi=-1):\n    model.train()\n    PATH = \"/kaggle/working/model_weights/best_expensive.pth\"\n    list_c = []\n    list_d = []\n    for noise_inp, clean in tqdm(train_loader, desc='Train'):\n        noise_inp = torch.flatten(noise_inp)\n        clean = torch.flatten(clean)\n        #print(clean.shape)\n        noise_inp, clean = noise_inp.numpy(), clean.numpy()\n        #list_c.append(noise_inp)\n        '''list_c.append(noise_inp)\n        list_d.append(clean)'''\n\n        noisy_input_fe = FeatureExtractor(noise_inp, windowLength=train_dataset.window_length, overlap=train_dataset.overlap,\n                                          sample_rate=train_dataset.sample_rate)\n        noise_spectrogram = noisy_input_fe.get_stft_spectrogram()\n        noise_magnitude = np.abs(noise_spectrogram)\n        \n        noise_ph = np.angle(noise_spectrogram)\n\n        # get the magnitude of the spectral\n        noise_magnitude = np.abs(noise_spectrogram)\n\n        # extract stft features from clean audio\n        clean_audio_fe = FeatureExtractor(clean, windowLength=train_dataset.window_length, overlap=train_dataset.overlap,\n                                          sample_rate=train_dataset.sample_rate)\n        clean_spectrogram = clean_audio_fe.get_stft_spectrogram()\n        # clean_spectrogram = cleanAudioFE.get_mel_spectrogram()\n\n  \n\n        # get the clean spectral magnitude\n        clean_magnitude = np.abs(clean_spectrogram)\n        #print(clean_magnitude.shape)\n        # clean_magnitude = 2 * clean_magnitude / np.sum(scipy.signal.hamming(self.window_length, sym=False))\n        clean_phase = np.angle(clean_spectrogram)\n        clean_magnitude = train_dataset._phase_aware_scaling(clean_magnitude, clean_phase, noise_ph)\n        #mean = np.mean(noise_magnitude)\n        #std = np.std(noise_magnitude)\n\n        #scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n        #noise_magnitude = scaler.fit_transform(noise_magnitude)\n\n        #clean_magnitude = scaler.transform(clean_magnitude)\n\n        noise_m_f = prepare_input_features(noise_magnitude, numSegments=16, numFeatures=129)\n        noise_m_f = np.transpose(noise_m_f, (2, 0, 1))\n        clean_magnitude = np.transpose(clean_magnitude, (1, 0))\n        #noise_ph = np.transpose(noise_ph, (1, 0))\n        noise_m_f = np.expand_dims(noise_m_f, axis=1)\n        clean_magnitude = np.expand_dims(clean_magnitude, axis=2)\n        clean_magnitude = np.expand_dims(clean_magnitude, 1)\n\n        \n        \n        x, y = torch.from_numpy(noise_m_f).to(device), torch.from_numpy(clean_magnitude).to(device)\n\n        #print(x.size())\n        optimizer.zero_grad()\n        output = model(x.float())\n\n        loss = loss_fn(output, y.float())\n        print(\"loss: \", loss)\n        wandb.log({\"loss\": loss})\n        #print(output.detach().numpy().shape)\n        denoisedAudioFullyConvolutional = revert_features_to_audio(output.cpu().detach().numpy(), noise_ph, noisy_input_fe)\n        inv_clean = revert_features_to_audio(y.cpu().detach().numpy(), noise_ph, noisy_input_fe)\n        #print(denoisedAudioFullyConvolutional.shape)\n        list_d.append(denoisedAudioFullyConvolutional)\n        list_c.append(inv_clean)\n        pesq_noisy = pesq(clean, noise_inp, train_dataset.sample_rate)\n        pesq_denoised = pesq(clean, denoisedAudioFullyConvolutional, train_dataset.sample_rate)\n        stoi_noisy = stoi(clean, noise_inp, train_dataset.sample_rate, extended=False)\n        stoi_denoised = stoi(clean, denoisedAudioFullyConvolutional, train_dataset.sample_rate, extended=False)\n        snr = getSNR_dB(getSNR(clean, noise_inp))\n        snr1 = getSNR_dB(getSNR(clean, denoisedAudioFullyConvolutional))\n        wandb.log({\"pesq\": pesq_denoised})\n        wandb.log({\"stoi\": stoi_denoised})\n        wandb.log({\"pesq_denoised - pesq_noisy\": pesq_denoised - pesq_noisy})\n        wandb.log({\"stoi_denoised - stoi_noisy\": stoi_denoised - stoi_noisy})\n        '''wandb.log({\"outputs\": wandb.Image(output)})\n        wandb.log({\"inputs_x\": wandb.Image(x)})\n        wandb.log({\"inputs_y\": wandb.Image(y)})'''\n        \n        if (stoi_denoised - stoi_noisy) + (pesq_denoised - pesq_noisy) *0.65 > best_diff_stoi:\n            print((stoi_denoised - stoi_noisy) + (pesq_denoised - pesq_noisy))\n            best_diff_stoi = (stoi_denoised - stoi_noisy) + (pesq_denoised - pesq_noisy) * 0.65\n            torch.save(model.state_dict(), PATH)\n            \n            \n        loss.backward()\n        optimizer.step()\n        i = 1 \n\n        \n        \n    return best_diff_stoi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Valid on epoch","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport soundfile as sf\nfrom pypesq import pesq\nfrom pystoi import stoi\nimport gc\n\ndef revert_features_to_audio(features, phase, noisy_input_fe, cleanMean=None, cleanStd=None):\n    # scale the outpus back to the original range\n    if cleanMean and cleanStd:\n        features = cleanStd * features + cleanMean\n\n    phase = np.transpose(phase, (1, 0))\n    features = np.squeeze(features)\n\n    # features = librosa.db_to_power(features)\n    features = features * np.exp(1j * phase)  # that fixes the abs() ope previously done\n\n    features = np.transpose(features, (1, 0))\n    return noisy_input_fe.get_audio_from_stft_spectrogram(features)\n\n\ndef evaluate_on_epoch(MyModel, loss_fn, val_loader, val_dataset):\n    model.eval()\n    total_MAE = 0\n    total_PESQ_noisy = 0\n    total_PESQ_denoised = 0\n    total_STOI_noisy = 0\n    total_STOI_denoised = 0\n    total_SNR_noisy = 0\n    total_SNR_denoised = 0\n    i = 0\n    \n    for noise_inp, clean in tqdm(val_loader, desc='Validation'):\n        noise_inp = torch.flatten(noise_inp)\n        clean = torch.flatten(clean)\n        #print(clean.shape)\n        noise_inp, clean = noise_inp.numpy(), clean.numpy()\n        #list_c.append(noise_inp)\n        '''list_c.append(noise_inp)\n        list_d.append(clean)'''\n\n        noisy_input_fe = FeatureExtractor(noise_inp, windowLength=val_dataset.window_length, overlap=val_dataset.overlap,\n                                          sample_rate=val_dataset.sample_rate)\n        noise_spectrogram = noisy_input_fe.get_stft_spectrogram()\n        noise_magnitude = np.abs(noise_spectrogram)\n        \n        noise_ph = np.angle(noise_spectrogram)\n\n        # get the magnitude of the spectral\n        noise_magnitude = np.abs(noise_spectrogram)\n\n        # extract stft features from clean audio\n        clean_audio_fe = FeatureExtractor(clean, windowLength=val_dataset.window_length, overlap=val_dataset.overlap,\n                                          sample_rate=val_dataset.sample_rate)\n        clean_spectrogram = clean_audio_fe.get_stft_spectrogram()\n        # clean_spectrogram = cleanAudioFE.get_mel_spectrogram()\n\n  \n\n        # get the clean spectral magnitude\n        clean_magnitude = np.abs(clean_spectrogram)\n        #print(clean_magnitude.shape)\n        # clean_magnitude = 2 * clean_magnitude / np.sum(scipy.signal.hamming(self.window_length, sym=False))\n        clean_phase = np.angle(clean_spectrogram)\n        clean_magnitude = val_dataset._phase_aware_scaling(clean_magnitude, clean_phase, noise_ph)\n        mean = np.mean(noise_magnitude)\n        std = np.std(noise_magnitude)\n\n        #scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n        #noise_magnitude = scaler.fit_transform(noise_magnitude)\n\n        #clean_magnitude = scaler.transform(clean_magnitude)\n\n        noise_m_f = prepare_input_features(noise_magnitude, numSegments=16, numFeatures=129)\n        noise_m_f = np.transpose(noise_m_f, (2, 0, 1))\n        clean_magnitude = np.transpose(clean_magnitude, (1, 0))\n        #noise_ph = np.transpose(noise_ph, (1, 0))\n        noise_m_f = np.expand_dims(noise_m_f, axis=1)\n        clean_magnitude = np.expand_dims(clean_magnitude, axis=2)\n        clean_magnitude = np.expand_dims(clean_magnitude, 1)\n\n        \n        \n        x, y = torch.from_numpy(noise_m_f).to(device), torch.from_numpy(clean_magnitude).to(device)\n\n        #print(x.size())\n        output = model(x.float())\n\n        loss = loss_fn(output, y.float())\n        \n        #wandb.log({\"loss\": loss})\n        #print(output.detach().numpy().shape)\n        denoisedAudioFullyConvolutional = revert_features_to_audio(output.cpu().detach().numpy(), noise_ph, noisy_input_fe)\n        inv_clean = revert_features_to_audio(y.cpu().detach().numpy(), noise_ph, noisy_input_fe)\n        #print(denoisedAudioFullyConvolutional.shape)\n        list_d.append(denoisedAudioFullyConvolutional)\n        list_c.append(inv_clean)\n        pesq_noisy = pesq(clean, noise_inp, val_dataset.sample_rate)\n        pesq_denoised = pesq(clean, denoisedAudioFullyConvolutional, val_dataset.sample_rate)\n        pesq_clean = pesq(clean, inv_clean, val_dataset.sample_rate)\n        st_noisy = stoi(clean, noise_inp, val_dataset.sample_rate, extended=False)\n        st_denoised = stoi(clean, denoisedAudioFullyConvolutional, val_dataset.sample_rate, extended=False)\n        st_clean = stoi(clean, inv_clean, val_dataset.sample_rate, extended=False)\n        snr_noisy = getSNR_dB(getSNR(clean, noise_inp))\n        snr_denoised = getSNR_dB(getSNR(clean, denoisedAudioFullyConvolutional))\n        '''print(\"pesq score noisy: \",score)\n        print(\"pesq score denoised: \",score1)\n        print(\"stoi value noisy: \", st)\n        print(\"stoi value denoised: \", st1)\n        print(\"snr value noisy: \", snr)\n        print(\"snr value denoised: \", snr1)\n        print(\"pesq score clean: \",score2)\n        print(\"stoi value clean: \", st2)'''\n        total_MAE += loss.item()\n        total_PESQ_noisy += pesq_noisy\n        total_PESQ_denoised += pesq_denoised\n        total_STOI_noisy += st_noisy\n        total_STOI_denoised += st_denoised\n        total_SNR_noisy += snr_noisy\n        total_SNR_denoised += snr_denoised\n\n        i += 1 \n        #if i >= 1:\n            #return list_c, list_d\n    pesq_denoised = total_PESQ_denoised / i\n    stoi_denoised =  total_STOI_denoised / i\n    pesq_noisy = total_PESQ_noisy / i\n    stoi_noisy = total_STOI_noisy / i\n    wandb.log({\"val pesq\": pesq_denoised})\n    wandb.log({\"val stoi\": stoi_denoised})\n    wandb.log({\"val pesq_denoised - pesq_noisy\": pesq_denoised - pesq_noisy})\n    wandb.log({\"val stoi_denoised - stoi_noisy\": stoi_denoised - stoi_noisy})\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/model_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Full training cycle","metadata":{}},{"cell_type":"code","source":"import torch\nimport wandb\nfrom mozilla_commonvoice import MozillaCommonVoiceDataset\nfrom urban_sound_8k import UrbanSound8K\n# from urban_sound_8K import UrbanSound8K\nfrom findataset import FinDataset\nimport warnings\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nwandb.init(project='speech_denoising_final_very_cheap')\nconfig = wandb.config\nconfig.learning_rate = 1e-3\n\nwarnings.filterwarnings(action='ignore')\n\nmozilla_basepath = \"../input/commonvoice2/commonvoice\"\nurbansound_basepath = \"../input/urbansound8k\"\n\n\n\n#model = MyModel3().to(device)\nfrom torch.optim import Adam\n#optimizer = Adam(list(model.parameters()), lr=(1e-5*2))\nwindowLength = 256\nconfig = {'windowLength': windowLength,\n          'overlap': round(0.25 * windowLength),\n          'fs': 16000,\n          'audio_max_duration': 0.8}\nnum_epochs = 10\n\nloss_fn = nn.L1Loss()\nn = 0\nbest_diff_stoi=-1\nfor epoch in range(num_epochs):\n    mcv = MozillaCommonVoiceDataset(mozilla_basepath, val_dataset_size=100)\n    clean_train_filenames, clean_val_filenames = mcv.get_train_val_filenames() \n\n    us8K = UrbanSound8K(urbansound_basepath, val_dataset_size=1000)\n    noise_train_filenames, noise_val_filenames = us8K.get_train_val_filenames()\n    \n    val_dataset = FinDataset(clean_val_filenames, noise_val_filenames, **config)  # Создание тренировочного, валидационного и тестового датасетов\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n    train_dataset = FinDataset(clean_train_filenames, noise_train_filenames, **config)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  #15\n\n    ## Create Test Set\n    clean_test_filenames = mcv.get_test_filenames() \n\n    noise_test_filenames = us8K.get_test_filenames()\n    noise_test_filenames = noise_test_filenames\n\n    test_dataset = FinDataset(clean_test_filenames, noise_test_filenames, **config)\n    print(\"epoch\", n)\n    if n > 6:\n        optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-3)\n    else:\n        optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-3)\n    n += 1\n    best_diff_stoi = train_on_epoch(model, loss_fn, optimizer, train_loader, train_dataset, best_diff_stoi)\n    #with torch.no_grad():\n        #evaluate_on_epoch(model, loss_fn, val_loader, val_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}